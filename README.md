
# ðŸ“š Multimodal RAG (Retrieval-Augmented Generation)

This repository demonstrates a **Multimodal RAG pipeline** - combining **text** and **image** data for retrieval-augmented generation.  
It enables querying and generating responses based on **both textual and visual content** from documents.

---

## ðŸ“– What is Multimodal RAG?

**Multimodal RAG** extends the traditional Retrieval-Augmented Generation approach by incorporating multiple data modalities (e.g., **images + text**).  
Instead of only retrieving text embeddings, the system also processes **image embeddings**, enabling richer context retrieval.

### âœ¨ Benefits
- Supports **text + image queries**.
- Retrieves more relevant context by considering multiple modalities.
- Enables building **vision-language models** on top of RAG pipeline
---

## ðŸ“„ Files Description

- **`docs.md`** â†’ Concept document explaining the architecture, flow, and steps.  
- **`Multimodal_RAG.ipynb`** â†’ Code notebook to run the pipeline end-to-end.  
- **`test.pdf`** â†’ PDF with text + images for testing.
- **`requirments.txt`** - Requiremnts to run the ipynb file.

---



